{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PRÁCTICA 1: Extracción de datos de una red social\n",
    "\n",
    "Esta práctica consiste en descargar comentarios de un subreddit de Reddit, almacenarlos en disco para su posterior análisis y realizar un simple procesamiento del corpus.\n",
    "Se vectoriza el corpus para obtener los términos más generales y los más repetidos.\n",
    "Por último se usa la Whoosh para indexar los documentos descargados y realizar búsquedas."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda de bash se crea el archivo `paraw.ini` en donde se almacenan las credenciales de acceso a la API, de esta forma se evita incluir las credenciales en el código.\n",
    "Entre corchetes se pone el identificador con el cual luego se hará referencia desde el código para cargar las credenciales.\n",
    "\n",
    "Dado que en esta práctica solo se hará uso de características de lectura de la API solo serán necesarias las credenciales de `client_id` y de `client_secret`.\n",
    "[[1]](https://praw.readthedocs.io/en/stable/getting_started/authentication.html#application-only-flows)\n",
    "Las otras credenciales son necesarias en caso de necesitar tener un contexto de usuario para poder hacer publicaciones de comentarios, upvotes, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "CLIENT_ID=\"\"\n",
    "CLIENT_SECRET=\"\"\n",
    "\n",
    "echo \"\n",
    "\n",
    "[tgine]\n",
    "client_id=$CLIENT_ID\n",
    "client_secret=$CLIENT_SECRET\n",
    "\" >> praw.ini"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se encuentran los imports necesarios para la ejecución del script."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import praw\n",
    "import json\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import Schema, TEXT, ID, BOOLEAN\n",
    "from whoosh.qparser import QueryParser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se definen constantes globales.\n",
    "\n",
    "* `subreddit_display_name` El subreddit elegido es [science](https://www.reddit.com/r/science/).\n",
    "* `collections_dir` La carpeta donde se guardaran las colecciones.\n",
    "* `collection_names` Los tres modos de obtener comentarios, se corresponden con los nombres de las colecciones\n",
    "* `reddit_client_user_agent` El _User-Agent_ que se usará con el cliente.\n",
    "* `min_document_frequency` Mínima frecuencia por término en los documentos.\n",
    "* `most_central_terms` Los términos más centrales que se mostrarán.\n",
    "* `most_repeated` Los términos más repetidos que se mostraran."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddit_display_name = \"science\"\n",
    "collections_dir = \"collections\"\n",
    "collection_names = [\"new\", \"hot\", \"rising\"]\n",
    "reddit_client_user_agent = \"python:com.example.gonzalocl1024.tgine:v1.0 (by /u/gonzalocl1024)\"\n",
    "\n",
    "min_document_frequency = 10\n",
    "most_central_terms = 50\n",
    "most_repeated = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se definen los atributos que se copiarán de cada uno de los objetos que devuelva la librería para cada uno de los comentarios.\n",
    "La función `copy_attributes` copia los atributos que son primitivos y serializables, en caso de que un atributo no exista saltará una excepción que será capturada.\n",
    "\n",
    "Cada una de las funciones copia un tipo de objeto respectivamente."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddit_attributes = [\n",
    "    \"display_name\",\n",
    "    \"title\",\n",
    "    \"active_user_count\",\n",
    "    \"subscribers\",\n",
    "    \"id\",\n",
    "    \"description\",\n",
    "    \"created_utc\",\n",
    "    \"name\"\n",
    "]\n",
    "\n",
    "submission_attributes = [\n",
    "    \"title\",\n",
    "    \"name\",\n",
    "    \"upvote_ratio\",\n",
    "    \"ups\",\n",
    "    \"score\",\n",
    "    \"id\",\n",
    "    \"created_utc\",\n",
    "    \"selftext\",\n",
    "    \"downs\",\n",
    "    \"url\"\n",
    "]\n",
    "\n",
    "comment_attributes = [\n",
    "    \"ups\",\n",
    "    \"id\",\n",
    "    \"score\",\n",
    "    \"body\",\n",
    "    \"downs\"\n",
    "]\n",
    "\n",
    "def copy_attributes(dest, src, attributes):\n",
    "    for attribute in attributes:\n",
    "        try:\n",
    "            dest[attribute] = src[attribute]\n",
    "        except KeyError:\n",
    "            i = None\n",
    "            n = None\n",
    "            try:\n",
    "                i = src[\"id\"]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            try:\n",
    "                n = src[\"name\"]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            print(\"\\n{}, {}, {}\".format(attribute, i, n))\n",
    "\n",
    "def copy_subreddit(subreddit):\n",
    "    dest = {}\n",
    "    copy_attributes(dest, subreddit, subreddit_attributes)\n",
    "    return dest\n",
    "\n",
    "def copy_submission(submission):\n",
    "    dest = {}\n",
    "    copy_attributes(dest, submission, submission_attributes)\n",
    "    return dest\n",
    "\n",
    "def copy_comment(comment):\n",
    "    dest = {}\n",
    "    copy_attributes(dest, comment, comment_attributes)\n",
    "    return dest\n",
    "\n",
    "def copy_author(dest, src):\n",
    "    if src.author:\n",
    "        dest[\"author\"] = src.author.name\n",
    "    else:\n",
    "        dest[\"author\"] = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se definen las funciones que se encargaran de descargarse los comentarios del subreddit.\n",
    "La primera función `retrieve_submissions` descarga todos los submissions que pueda y devuelve una lista de objetos submission.\n",
    "\n",
    "La función siguiente función `retrieve_comments`, se encarga de descargar todos los comentarios de todos los submissions que se han obtenido en la función anterior.\n",
    "Para cada submision se copia el contenido del post inicial y además con la línea `submission.comments.replace_more(limit=None)`, el wrapper se encarga de [descargar todos los comentarios](https://praw.readthedocs.io/en/stable/tutorials/comments.html#the-replace-more-method) de ese submision.\n",
    "Se itera sobre todos los comentarios se copian de los objetos creados por el wrapper a diccionarios de Python y se almacenan en la colección que se pasó por parámetro.\n",
    "El wrapper convierte el campo autor en un objeto por lo que se copia solo el username del autor, para que posteriormente sea posible serializar los datos copiados.\n",
    "\n",
    "Por último la función `get_collection` se encarga de llamar a las dos funciones anteriores.\n",
    "Primero copia la información del subreddit, luego se descarga la colección que se le pasa por parámetro, y después se descargan todos los comentarios.\n",
    "\n",
    "En la linea `getattr(subreddit, name)(limit=None)`, el parámetro `name` corresponde con el nombre de la colección (_new_, _hot_, _rising_), y con el parámetro `limit=None` se le indica que se descargue el máximo número de submision de esa colección posible.\n",
    "\n",
    "La estructura del diccionario devuelto es la siguiente:\n",
    "\n",
    "```\n",
    "{'subreddit': {<información sobre el subreddit>},\n",
    " 'submissions': [{'submission': {<información sobre el submission>}, 'comments': [<lista de comentarios>]},\n",
    "                 {'submission': {<información sobre el submission>}, 'comments': [<lista de comentarios>]},\n",
    "                 ...]}\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_submissions(submissions):\n",
    "\n",
    "    submission_list = []\n",
    "    i = 0\n",
    "\n",
    "    for submission in submissions:\n",
    "        submission_list.append(submission)\n",
    "\n",
    "        i += 1\n",
    "        print(\"\\rRetrieved submissions: {}\".format(i), end=\"\")\n",
    "\n",
    "    print()\n",
    "    return submission_list\n",
    "\n",
    "def retrieve_comments(collection, submissions):\n",
    "\n",
    "    i = 0\n",
    "    total_submissions = len(submissions)\n",
    "    total_comments = 0\n",
    "\n",
    "    collection[\"submissions\"] = []\n",
    "\n",
    "    for submission in submissions:\n",
    "\n",
    "        _ = submission.title\n",
    "        submission_copy = copy_submission(vars(submission))\n",
    "        copy_author(submission_copy, submission)\n",
    "\n",
    "        print(\" Next comment batch: {:<35}\".format(submission.num_comments), end=\"\")\n",
    "\n",
    "        comments = []\n",
    "        submission.comments.replace_more(limit=None)\n",
    "\n",
    "        for comment in submission.comments.list():\n",
    "            _ = comment.body\n",
    "            comment_copy = copy_comment(vars(comment))\n",
    "            copy_author(comment_copy, comment)\n",
    "            comments.append(comment_copy)\n",
    "\n",
    "        collection[\"submissions\"].append({\n",
    "            \"submission\": submission_copy,\n",
    "            \"comments\": comments\n",
    "        })\n",
    "\n",
    "        i += 1\n",
    "        total_comments += len(comments)\n",
    "        print(\"\\rRetrieved comments: {} (Submissions: {}/{})\".format(total_comments, i, total_submissions), end=\"\")\n",
    "\n",
    "    print()\n",
    "\n",
    "def get_collection(subreddit, name):\n",
    "\n",
    "    _ = subreddit.title\n",
    "\n",
    "    submissions = getattr(subreddit, name)(limit=None)\n",
    "\n",
    "    collection = {}\n",
    "\n",
    "    # add subreddit info and date\n",
    "    collection[\"collection_name\"] = name\n",
    "    collection[\"subreddit\"] = copy_subreddit(vars(subreddit))\n",
    "    collection[\"date\"] = datetime.datetime.now().isoformat()\n",
    "\n",
    "    # retrieve submissions list\n",
    "    submission_list = retrieve_submissions(submissions)\n",
    "\n",
    "    # retrieve comments\n",
    "    retrieve_comments(collection, submission_list)\n",
    "\n",
    "    return collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función `get_collections` se encarga de cargar las colecciones.\n",
    "En primer lugar se comprueba si la colección ya ha sido descargada y está almacenada en disco, en tal caso se carga de disco el JSON de la colección.\n",
    "\n",
    "Si por el contrario la colección no se encontrase en disco se descargaría haciendo uso de las funciones descritas anteriormente.\n",
    "En este caso se guardará la colección en disco en formato JSON para ser usada en futuras ejecuciones.\n",
    "\n",
    "Toma por parámetros el nombre del subreddit a descargar y los nombres de los modos de descargar las colecciones (_new_, _hot_, _rising_) que se corresponden con los nombres de las colecciones.\n",
    "Devuelve un diccionario con una clave por colección, cada cual contiene la colección correspondiente.\n",
    "\n",
    "A la hora de instanciar el objeto cliente de la API de reddit (`praw.Reddit(\"tgine\", user_agent=reddit_client_user_agent)`), en vez de pasarle por parámetros las credenciales, se le pasa el identificador para que lea las credenciales que se almacenaron en el archivo `praw.ini` anteriormente."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_collection_path(collection_dir, display_name, collection_name):\n",
    "    return os.path.join(collection_dir, \"{}_{}.json\".format(display_name, collection_name))\n",
    "\n",
    "def get_collections(display_name, names):\n",
    "\n",
    "    reddit_client = None\n",
    "    subreddit = None\n",
    "\n",
    "    os.makedirs(collections_dir, exist_ok=True)\n",
    "\n",
    "    collections = {}\n",
    "\n",
    "    for collection_name in names:\n",
    "\n",
    "        collection_filename = get_collection_path(collections_dir, display_name, collection_name)\n",
    "\n",
    "        if os.path.isfile(collection_filename):\n",
    "            # read collection\n",
    "            print(\"Using saved collection: {}\".format(collection_filename))\n",
    "            with open(collection_filename) as collection_file:\n",
    "                collection = json.load(collection_file)\n",
    "\n",
    "            total_comments = 0\n",
    "            for submission in collection[\"submissions\"]:\n",
    "                total_comments += len(submission[\"comments\"])\n",
    "            print(\"  Submissions: {}, Comments: {}\".format(len(collection[\"submissions\"]), total_comments))\n",
    "\n",
    "        else:\n",
    "            # retrieve collection\n",
    "\n",
    "            if not reddit_client:\n",
    "                reddit_client = praw.Reddit(\"tgine\", user_agent=reddit_client_user_agent)\n",
    "                subreddit = reddit_client.subreddit(display_name)\n",
    "\n",
    "            collection = get_collection(subreddit, collection_name)\n",
    "\n",
    "            print(\"Saving collection: {}\".format(collection_filename))\n",
    "            with open(collection_filename, \"w\") as collection_file:\n",
    "                json.dump(collection, collection_file)\n",
    "\n",
    "        collections[collection_name] = collection\n",
    "\n",
    "    return collections"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función `extract_corpus` se encarga de tomar las colecciones anteriores, que consisten en diccionarios con la información del subrredit, submissions y comentarios y convertir eso en un diccionario con una estructura más simple que contenga simplemente el texto de las submissions y los comentarios.\n",
    "\n",
    "Para cada colección se crea un corpus distinto, en cuanto al post inicial (submission), el texto del título y el contenido del post se concatenan.\n",
    "El contenido de los comentarios se incluye todo en la misma lista.\n",
    "Para cada colección se crea una lista donde cada elemento de la lista será un documento.\n",
    "\n",
    "Por último se descartan todos los comentarios que sean del usuario `AutoModerator`, que es un bot.\n",
    "Se descartan los mensajes del bot porque en todos los submisions el primer comentario es de este bot y en todas las ocasiones es el mismo mensaje ([ejemplo](https://www.reddit.com/r/science/comments/nyhcip/study_employees_who_experienced_higher_levels_of/h1k2vhx/)), lo que añade ruido en las colecciones, sobre todo en las colecciones en las que por lo general no hay muchos comentarios por submission.\n",
    "Como por ejemplo en la colección _new_ donde al estar menos tiempo publicadas las submissions tienen menos comentarios.\n",
    "\n",
    "La estructura del diccionario devuelto es la siguiente:\n",
    "\n",
    "```\n",
    "{'new': [<lista de documentos>],\n",
    " 'hot': [<lista de documentos>],\n",
    " 'rising': [<lista de documentos>]}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_corpus(collections):\n",
    "\n",
    "    corpus = {}\n",
    "\n",
    "    for collection in collections:\n",
    "        corpus[collection] = []\n",
    "\n",
    "        for submission in collections[collection][\"submissions\"]:\n",
    "\n",
    "            corpus[collection].append(\"{} {}\".format(submission[\"submission\"][\"title\"],\n",
    "                                                     submission[\"submission\"][\"selftext\"]))\n",
    "\n",
    "            for comment in submission[\"comments\"]:\n",
    "\n",
    "                # AutoModerator is a bot discard its messages\n",
    "                if not comment[\"author\"] == \"AutoModerator\":\n",
    "                    corpus[collection].append(comment[\"body\"])\n",
    "\n",
    "    return corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se cargan las colecciones y seguidamente se extrae el corpus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subreddit_collections = get_collections(subreddit_display_name, collection_names)\n",
    "collections_corpus = extract_corpus(subreddit_collections)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente función se vectoriza el corpus extraído anteriormente y se muestran los términos más centrales y los más repetidos.\n",
    "\n",
    "Para la vectorización al objeto vectorizador se le pasa el parámetro `min_df=min_doc_freq` con el que se le indica que descarte los términos menos frecuentes que `min_doc_freq`.\n",
    "Con el parámetro `max_features=most_rep` se le indica que se quede solo con los `most_rep` términos, los más repetidos del corpus.\n",
    "Como se indica en la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "Con `vectorizer.get_feature_names()` se obtienen los términos más repetidos de la colección.\n",
    "\n",
    "Con `np.sum(tfidf.toarray(), axis=0)` se obtienen las puntuaciones de los términos más centrales, calculadas como la suma de todas sus puntuaciones a través de todos los documentos.\n",
    "La puntuación resultante en cada posición de la lista se corresponde con el término devuelto por el método `get_feature_names`.\n",
    "Para obtener los términos más centrales se tienen que obtener los índices correspondientes con las puntuaciones más altas, para ello se usa el método `np.argsort`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_central_repeated(corpus, min_doc_freq, most_central, most_rep):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", min_df=min_doc_freq, max_features=most_rep)\n",
    "    tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    most_repeated_terms = vectorizer.get_feature_names()\n",
    "\n",
    "    central_terms_score = np.sum(tfidf.toarray(), axis=0)\n",
    "    central_terms_indexes = np.argsort(central_terms_score)[-most_central:]\n",
    "    central_terms = [most_repeated_terms[i] for i in central_terms_indexes]\n",
    "    central_terms.reverse()\n",
    "\n",
    "    print(\"{} most central terms\".format(most_central))\n",
    "    pprint(central_terms, width=100, compact=True)\n",
    "    print()\n",
    "\n",
    "    print(\"{} most repeated terms\".format(most_rep))\n",
    "    pprint(most_repeated_terms, width=100, compact=True)\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestran los términos más centrales y más repetidos para cada función."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for collection_corpus in collections_corpus:\n",
    "\n",
    "    print(\"Collection: {}\".format(collection_corpus))\n",
    "    print_central_repeated(collections_corpus[collection_corpus], min_document_frequency, most_central_terms, most_repeated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se realiza el apartado opcional _Utilizad Whoosh para indexar los documentos_.\n",
    "\n",
    "Las siguientes funciones servirán para hacer uso del índice de documentos.\n",
    "La función `get_index` se usa para cargar el índice, en caso de que el índice ya haya sido creado y exista en disco, ese índice es cargado, en caso contrario se crea.\n",
    "\n",
    "Para crear el índice primero se define el esquema, el esquema se compone de los campos:\n",
    "\n",
    "*  _collection_: indica a la colección que pertenece (_new_, _hot_, _rising_).\n",
    "*  *submission_id*: es el identificador del submission, puede servir para filtrar por submission.\n",
    "*  _author_: autor del contenido.\n",
    "*  _content_: el texto del submission o comentario.\n",
    "*  *is_submission*: indica si el documento indexado es submission o comentario.\n",
    "\n",
    "Añadiendo `stored=True` se indica que se almacene el dato para que a la hora de hacer consultas pueda ser consultado y mostrado, en caso contrario solo se podrán hacer búsquedas con ese dato, pero no mostrarlo.\n",
    "\n",
    "Seguidamente se crea el objeto indexador, y se llama a la función `add_collections` que añadirá todos los documentos de todas las colecciones al índice.\n",
    "\n",
    "Por último la función `print_results` se puede usar para mostrar los resultados obtenidos en una consulta de una forma legible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_collections(writer, collections):\n",
    "\n",
    "    total_submissions = 0\n",
    "    submissions = 0\n",
    "    for collection in collections:\n",
    "        total_submissions += len(collections[collection][\"submissions\"])\n",
    "\n",
    "    for collection in collections:\n",
    "        for submission in collections[collection][\"submissions\"]:\n",
    "\n",
    "            writer.add_document(collection=collection,\n",
    "                                submission_id=submission[\"submission\"][\"id\"],\n",
    "                                author=submission[\"submission\"][\"author\"],\n",
    "                                content=\"{}\\n{}\".format(submission[\"submission\"][\"title\"],\n",
    "                                                        submission[\"submission\"][\"selftext\"]),\n",
    "                                is_submission=True)\n",
    "\n",
    "            total_comments = len(submission[\"comments\"])\n",
    "            comments = 0\n",
    "            submissions += 1\n",
    "\n",
    "            for comment in submission[\"comments\"]:\n",
    "                writer.add_document(collection=collection,\n",
    "                                    submission_id=submission[\"submission\"][\"id\"],\n",
    "                                    author=comment[\"author\"],\n",
    "                                    content=comment[\"body\"],\n",
    "                                    is_submission=False)\n",
    "\n",
    "                comments += 1\n",
    "                print(\"\\rIndexed: Submissions {}/{}; Comments {}/{}               \".format(submissions,\n",
    "                                                                                           total_submissions,\n",
    "                                                                                           comments,\n",
    "                                                                                           total_comments), end=\"\")\n",
    "    print()\n",
    "\n",
    "def get_index(index_path, collections):\n",
    "\n",
    "    if os.path.exists(index_path):\n",
    "        print(\"Using saved index: {}\".format(index_path))\n",
    "        return open_dir(index_path)\n",
    "\n",
    "    print(\"Creating index: {}\".format(index_path))\n",
    "    os.mkdir(index_path)\n",
    "\n",
    "    schema = Schema(collection=TEXT(stored=True),\n",
    "                    submission_id=ID(stored=True),\n",
    "                    author=TEXT(stored=True),\n",
    "                    content=TEXT(stored=True),\n",
    "                    is_submission=BOOLEAN(stored=True))\n",
    "\n",
    "    index = create_in(index_path, schema)\n",
    "\n",
    "    writer = index.writer()\n",
    "    add_collections(writer, collections)\n",
    "    writer.commit()\n",
    "\n",
    "    return index\n",
    "\n",
    "def print_results(results):\n",
    "\n",
    "    print(\"Search runtime: {}\".format(results.runtime))\n",
    "    print(\"Total results: {} (showing {})\\n\".format(results.estimated_length(),\n",
    "                                                    results.scored_length()))\n",
    "\n",
    "    for result in results:\n",
    "        print(\"{:<20}{}\".format(result[\"author\"], result[\"content\"].strip()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se carga el índice haciendo uso de las funciones antes descritas.\n",
    "Primero se carga el índice y luego se obtiene el objeto `index_searcher` que es necesario para poder hacer consultas al índice.\n",
    "\n",
    "También se instancia un objeto `QueryParser` que servirá para poder hacer consultas sin necesidad de crear un objeto consulta con código Python.\n",
    "Con este objeto se puede hacer una consulta a partir de una consulta textual usando la sintaxis adecuada.\n",
    "Al objeto `QueryParser` hay que pasarle por parámetros el campo por defecto que se usará para buscar (en este caso 'content') y el esquema del índice."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collections_index = get_index(\"index\", subreddit_collections)\n",
    "index_searcher = collections_index.searcher()\n",
    "query_parser = QueryParser(\"content\", collections_index.schema)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación se muestra un ejemplo de consulta al índice.\n",
    "Primero se obtiene el objeto de consulta a partir de la cadena de consulta usando el objeto `QueryParser`.\n",
    "Seguidamente se hace la búsqueda en el índice con el método `search` y se muestran los resultados.\n",
    "\n",
    "La sintaxis que se ha de usar para la cadena de consulta son pares de campos seguidos del valor que se quiere buscar en ese campo, separados por `:`.\n",
    "En caso de que no se indique el campo en el que se quiere buscar, se buscará en el campo por defecto especificado en la instanciación del objeto `QueryParser`.\n",
    "\n",
    "```\n",
    "<consulta en campo por defecto> <campo>:<consulta en el campo> <campo>:<consulta en el campo> ...\n",
    "```\n",
    "\n",
    "Especificar el campo donde buscar es una opción del lenguaje de consulta, el lenguaje proporciona más características como operaciones lógicas (AND y OR)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_str = \"scientists is_submission:true\"\n",
    "\n",
    "query = query_parser.parse(query_str)\n",
    "search_results = index_searcher.search(query)\n",
    "print_results(search_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para finalizar el índice se cierra para liberar los recursos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index_searcher.close()\n",
    "collections_index.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}