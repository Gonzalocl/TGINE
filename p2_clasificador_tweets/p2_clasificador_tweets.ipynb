{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import tweepy\n",
    "import operator\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_users = [\"astro_ricky\", \"AstroVicGlover\"]\n",
    "different_users = [\"DaveMustaine\", \"jack\"]\n",
    "\n",
    "twitter_api_key = \"\"\n",
    "twitter_api_secret = \"\"\n",
    "\n",
    "collections_dir = \"collections\"\n",
    "max_pages = 100\n",
    "max_page_fail_retries = 3\n",
    "max_consecutive_pages_failed = 3\n",
    "request_wait = 1\n",
    "page_fail_wait = 30\n",
    "\n",
    "test_set_percentage = 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_attributes = [\n",
    "    \"display_text_range\",\n",
    "    \"entities\",\n",
    "    \"favorite_count\",\n",
    "    \"full_text\",\n",
    "    \"id\",\n",
    "    \"retweet_count\",\n",
    "    \"truncated\"\n",
    "]\n",
    "\n",
    "def copy_attributes(dest, src, attributes):\n",
    "    for attribute in attributes:\n",
    "        try:\n",
    "            dest[attribute] = src[attribute]\n",
    "        except KeyError:\n",
    "            i = None\n",
    "            try:\n",
    "                i = src[\"id\"]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            print(\"KeyError: {}, {}\".format(attribute, i))\n",
    "\n",
    "def copy_tweet(tweet):\n",
    "    dest = {}\n",
    "    copy_attributes(dest, vars(tweet), tweet_attributes)\n",
    "    dest[\"author_screen_name\"] = tweet.author.screen_name\n",
    "    dest[\"created_at_timestamp\"] = tweet.created_at.timestamp()\n",
    "    dest[\"retweeted_status\"] = hasattr(tweet, \"retweeted_status\")\n",
    "    if hasattr(tweet, \"extended_entities\"):\n",
    "        dest[\"extended_entities\"] = tweet.extended_entities\n",
    "    return dest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_comments(twitter_client, name):\n",
    "\n",
    "    tweets = []\n",
    "    page = 1\n",
    "    page_fail_retries = 0\n",
    "    consecutive_pages_failed = 0\n",
    "\n",
    "    print(\"Retrieving {}\".format(name))\n",
    "\n",
    "    while page < max_pages and consecutive_pages_failed < max_consecutive_pages_failed:\n",
    "\n",
    "        try:\n",
    "            statuses = twitter_client.user_timeline(id=name, tweet_mode=\"extended\", page=page)\n",
    "        except tweepy.TweepError:\n",
    "            statuses = None\n",
    "\n",
    "        if statuses:\n",
    "            for status in statuses:\n",
    "                tweets.append(copy_tweet(status))\n",
    "\n",
    "            page += 1\n",
    "            page_fail_retries = 0\n",
    "            consecutive_pages_failed = 0\n",
    "\n",
    "            print(\"Retrieved Tweets: {} (Pages: {}/{})\".format(len(tweets), page, max_pages), end=\"\\r\")\n",
    "            time.sleep(request_wait)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if page_fail_retries < max_page_fail_retries:\n",
    "                page_fail_retries += 1\n",
    "\n",
    "                print(\"Failed to retrieve page {} attempts {}/{}, sleeping {} seconds\".format(\n",
    "                    page,\n",
    "                    page_fail_retries,\n",
    "                    max_page_fail_retries,\n",
    "                    page_fail_wait))\n",
    "                print(\"Retrieved Tweets: {} (Pages: {}/{})\".format(len(tweets), page, max_pages), end=\"\\r\")\n",
    "\n",
    "            else:\n",
    "                consecutive_pages_failed += 1\n",
    "\n",
    "                print(\"{} consecutive failed attempts on page {}, skipping page {}, (Consecutive pages failed {}/{}), sleeping {} seconds\".format(\n",
    "                    max_page_fail_retries,\n",
    "                    page, page,\n",
    "                    consecutive_pages_failed,\n",
    "                    max_consecutive_pages_failed,\n",
    "                    page_fail_wait))\n",
    "                print(\"Retrieved Tweets: {} (Pages: {}/{})\".format(len(tweets), page, max_pages), end=\"\\r\")\n",
    "\n",
    "                page += 1\n",
    "                page_fail_retries = 0\n",
    "\n",
    "            time.sleep(page_fail_wait)\n",
    "\n",
    "    print()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def get_collection(twitter_client, name):\n",
    "\n",
    "    tweets = retrieve_comments(twitter_client, name)\n",
    "\n",
    "    collection = {\n",
    "        \"collection_name\": name,\n",
    "        \"tweets\": tweets,\n",
    "        \"date\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    return collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_collection_path(collection_dir, name):\n",
    "    return os.path.join(collection_dir, \"{}.json\".format(name))\n",
    "\n",
    "def get_collections(names):\n",
    "\n",
    "    twitter_client = None\n",
    "    collections = {}\n",
    "\n",
    "    os.makedirs(collections_dir, exist_ok=True)\n",
    "\n",
    "    for collection_name in names:\n",
    "\n",
    "        collection_filename = get_collection_path(collections_dir, collection_name)\n",
    "\n",
    "        if os.path.isfile(collection_filename):\n",
    "            # read collection\n",
    "            print(\"Using saved collection: {}\".format(collection_filename))\n",
    "            with open(collection_filename) as collection_file:\n",
    "                collection = json.load(collection_file)\n",
    "\n",
    "            print(\"  Tweets: {}\".format(len(collection[\"tweets\"])))\n",
    "\n",
    "        else:\n",
    "            # retrieve collection\n",
    "\n",
    "            if not twitter_client:\n",
    "                twitter_auth = tweepy.AppAuthHandler(twitter_api_key, twitter_api_secret)\n",
    "                twitter_client = tweepy.API(twitter_auth)\n",
    "\n",
    "            collection = get_collection(twitter_client, collection_name)\n",
    "\n",
    "            print(\"Saving collection: {}\".format(collection_filename))\n",
    "            with open(collection_filename, \"w\") as collection_file:\n",
    "                json.dump(collection, collection_file)\n",
    "\n",
    "        collections[collection_name] = collection\n",
    "\n",
    "    return collections"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twitter_collections = get_collections(similar_users + different_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_training_test(corpus, test_percentage):\n",
    "\n",
    "    corpus_sorted = list(map(\n",
    "        operator.itemgetter('text'),\n",
    "        sorted(corpus, key=operator.itemgetter('timestamp'))\n",
    "    ))\n",
    "\n",
    "    training_size = int(len(corpus_sorted) * (1-test_percentage))\n",
    "\n",
    "    return corpus_sorted[:training_size], corpus_sorted[training_size:]\n",
    "\n",
    "def extract_corpus(collections):\n",
    "\n",
    "    corpus_sets = {}\n",
    "\n",
    "    for collection in collections:\n",
    "\n",
    "        corpus = []\n",
    "\n",
    "        for tweet in collections[collection][\"tweets\"]:\n",
    "            start = tweet[\"display_text_range\"][0]\n",
    "            end = tweet[\"display_text_range\"][1]\n",
    "\n",
    "            if not tweet[\"retweeted_status\"] and (end-start) != 0:\n",
    "                corpus.append({'text': tweet[\"full_text\"][start:end], 'timestamp': tweet['created_at_timestamp']})\n",
    "\n",
    "        training, test = split_training_test(corpus, test_set_percentage)\n",
    "        corpus_sets[collection] = {'training': training, 'test': test}\n",
    "\n",
    "    return corpus_sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collections_corpus_sets = extract_corpus(twitter_collections)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorize_corpus(corpus, collections):\n",
    "\n",
    "    vectors = {'collections': collections,\n",
    "               'label_map': {},\n",
    "               'training': {'labels': []},\n",
    "               'test': {'labels': []}}\n",
    "\n",
    "    training_corpus = []\n",
    "    test_corpus = []\n",
    "    label = 0\n",
    "\n",
    "    for collection in collections:\n",
    "\n",
    "        training_corpus += corpus[collection]['training']\n",
    "        test_corpus += corpus[collection]['test']\n",
    "\n",
    "        vectors['training']['labels'] += [label for _ in range(0, len(corpus[collection]['training']))]\n",
    "        vectors['test']['labels'] += [label for _ in range(0, len(corpus[collection]['test']))]\n",
    "\n",
    "        vectors['label_map'][label] = collection\n",
    "        label += 1\n",
    "\n",
    "    random.Random(1024).shuffle(training_corpus)\n",
    "    random.Random(1024).shuffle(vectors['training']['labels'])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors['training']['vectors'] = vectorizer.fit_transform(training_corpus)\n",
    "    vectors['test']['vectors'] = vectorizer.transform(test_corpus)\n",
    "\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_users_vectors = vectorize_corpus(collections_corpus_sets, similar_users)\n",
    "\n",
    "different_users_vectors = vectorize_corpus(collections_corpus_sets, different_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(ground_truth, prediction):\n",
    "\n",
    "    confusion_matrix = [[0, 0], [0, 0]]\n",
    "\n",
    "    for gt, p in zip(ground_truth, prediction):\n",
    "        confusion_matrix[gt][p] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "def calculate_accuracy(ground_truth, prediction):\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for gt, p in zip(ground_truth, prediction):\n",
    "        if gt == p:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "def print_confusion_matrix_accuracy(confusion_matrix, accuracy, label_map):\n",
    "\n",
    "    print('{:<16}{:<16}{:<16}'.format(' ', label_map[0], label_map[1]))\n",
    "    print('{:<16}{:<16}{:<16}'.format(label_map[0], confusion_matrix[0][0], confusion_matrix[0][1]))\n",
    "    print('{:<16}{:<16}{:<16}  Accuracy: {:0.3f}'.format(label_map[1], confusion_matrix[1][0], confusion_matrix[1][1], accuracy))\n",
    "    print()\n",
    "\n",
    "def train_predict(vectors, *args, **kwargs):\n",
    "\n",
    "    print('Configuration:', args, kwargs)\n",
    "\n",
    "    classifier = svm.SVC(*args, **kwargs)\n",
    "    classifier.fit(vectors['training']['vectors'], vectors['training']['labels'])\n",
    "    prediction = classifier.predict(vectors['test']['vectors'])\n",
    "\n",
    "    confusion_matrix = calculate_confusion_matrix(vectors['test']['labels'], prediction)\n",
    "    accuracy = calculate_accuracy(vectors['test']['labels'], prediction)\n",
    "\n",
    "    print_confusion_matrix_accuracy(confusion_matrix, accuracy, vectors['label_map'])\n",
    "\n",
    "    return confusion_matrix, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_predict(similar_users_vectors)\n",
    "train_predict(different_users_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}