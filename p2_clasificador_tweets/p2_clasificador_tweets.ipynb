{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PRACTICA 2: Clasificador de tweets\n",
    "\n",
    "Esta práctica consiste en descargarse tweets de 4 usuarios con amplia actividad, de los cuales 2 han de ser similares y 2 diferentes.\n",
    "A continuación se deben vectorizar y entrenar una SVM para realizar la clasificación de los tweets.\n",
    "Por último se hace análisis de sentimiento.\n",
    "\n",
    "En la siguiente celda se encuentran los imports necesarios para la ejecución del script."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import tweepy\n",
    "import operator\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se definen constantes globales.\n",
    "\n",
    "* Los usuarios similares escogidos son los astronautas [Ricky Arnold](https://twitter.com/astro_ricky) y [Victor Glover](https://twitter.com/AstroVicGlover).\n",
    "Los usuarios diferentes seleccionados son [Dave Mustaine](https://twitter.com/davemustaine) cantante de una banda de rock y [Jack Dorsey](https://twitter.com/jack) empresario tecnológico.\n",
    "\n",
    "* Para hacer uso de la API de Tweeter defino la clave y el secreto. Ya que solo se va a hacer uso de características de lectura y no de escritura (publicación de tweets, me gusta, etc.) solo son necesarios estos credenciales.\n",
    "La librería se ocupará de obtener un _bearer token_.\n",
    "[[1]](https://developer.twitter.com/en/docs/authentication/oauth-2-0)\n",
    "[[2]](https://docs.tweepy.org/en/stable/auth_tutorial.html#oauth-2-authentication)\n",
    "\n",
    "* Unas constantes que definirán el comportamiento a la hora de obtener los tweets.\n",
    "    * La carpeta donde se guardaran las colecciones de tweets.\n",
    "    * La cantidad de tweets que se solicitarán a la API en cada llamada. Se establece a 200 que es el maximo indicado en la [documentación](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-user_timeline).\n",
    "\n",
    "    Se establecen unos límites a la hora de obtener páginas de tweets, para que la descarga no se quede de forma indefinida haciendo llamadas a la API:\n",
    "    * Como mucho se descargarán 50 páginas. Aunque en la [documentación](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-user_timeline) se indica que el maximo de de tweets que se pueden descargar son 3200 en cada pagina es posible que no se obtengan el numero de tweets solicitados devido a que hayan sido borrados. De esta forma con 50 páginas como máximo hay de sobra para obtener los máximos tweets posibles.\n",
    "    * Los máximos intentos para descargar una página serán 3.\n",
    "    * En caso de que se descarten 3 páginas consecutivas la descarga terminará.\n",
    "\n",
    "    Se establecen unos retardos para no pasar el límite de llamadas permitidas por la API:\n",
    "    * Se incluye un retardo por cada llamada a la API, el límite de solicitudes de obtención de tweets (`lists/statuses`) es de [900 solicitudes por cada 15 minutos](https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits). 15 minutos son 900 segundos por lo que el retardo será de un segundo por cada llamada.\n",
    "    * En caso de que haya un fallo de página se establece un retardo de 30 segundos para asegurarse de que no se sobrepasa el límite establecido por la API.\n",
    "\n",
    "* Porcentaje de los tweets que se reservaran para el conjunto de test.\n",
    "\n",
    "* Una semilla para que los números aleatorios sean los mismos, más adelante será necesario."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_users = [\"astro_ricky\", \"AstroVicGlover\"]\n",
    "different_users = [\"DaveMustaine\", \"jack\"]\n",
    "\n",
    "twitter_api_key = \"\"\n",
    "twitter_api_secret = \"\"\n",
    "\n",
    "collections_dir = \"collections\"\n",
    "tweets_page = 200\n",
    "max_pages = 50\n",
    "max_page_fail_retries = 3\n",
    "max_consecutive_pages_failed = 3\n",
    "request_wait = 1\n",
    "page_fail_wait = 30\n",
    "\n",
    "test_set_percentage = 0.3\n",
    "\n",
    "seed = 1024"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda defino los atributos que se copiarán de cada uno de los objetos que devuelva la librería para cada uno de los tweets.\n",
    "La función `copy_attributes` copia los atributos que son primitivos y serializables, en caso de que un atributo no exista saltará una excepción que será capturada.\n",
    "\n",
    "La función `copy_tweet` copia un objeto de tweet devuelto por la librería a un diccionario de Python, hace uso de la función anterior para copiar los atributos primitivos y copia los atributos que han sido convertidos en objetos por la librería de forma que sean serializables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_attributes = [\n",
    "    \"display_text_range\",\n",
    "    \"entities\",\n",
    "    \"favorite_count\",\n",
    "    \"full_text\",\n",
    "    \"id\",\n",
    "    \"retweet_count\",\n",
    "    \"truncated\"\n",
    "]\n",
    "\n",
    "def copy_attributes(dest, src, attributes):\n",
    "    for attribute in attributes:\n",
    "        try:\n",
    "            dest[attribute] = src[attribute]\n",
    "        except KeyError:\n",
    "            i = None\n",
    "            try:\n",
    "                i = src[\"id\"]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            print(\"KeyError: {}, {}\".format(attribute, i))\n",
    "\n",
    "def copy_tweet(tweet):\n",
    "    dest = {}\n",
    "    copy_attributes(dest, vars(tweet), tweet_attributes)\n",
    "    dest[\"author_screen_name\"] = tweet.author.screen_name\n",
    "    dest[\"created_at_timestamp\"] = tweet.created_at.timestamp()\n",
    "    dest[\"retweeted_status\"] = hasattr(tweet, \"retweeted_status\")\n",
    "    if hasattr(tweet, \"extended_entities\"):\n",
    "        dest[\"extended_entities\"] = tweet.extended_entities\n",
    "    return dest"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función `retrieve_comments` en la siguiente celda, hace la descarga de los tweets para un usuario que se le pase por parámetro.\n",
    "Se tendrán en cuenta los limites y los retardos descritos anteriormente cuando sean necesarios.\n",
    "\n",
    "A la hora de llamar a la API desde el cliente `twitter_client` se le pasa el parámetro `tweet_mode=\"extended\"` para que obtenga los [tweets completos en vez de estar truncados](https://docs.tweepy.org/en/stable/extended_tweets.html#standard-api-methods), en caso contrario se truncarían por compatibilidad.\n",
    "También se le pasa el parámetro `count=tweets_page` que indica cuántos tweets se quieren obtener por cada página.\n",
    "\n",
    "La función devuelve una lista de diccionarios que son copias de los objetos que ha obtenido la librería."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def retrieve_comments(twitter_client, name):\n",
    "\n",
    "    tweets = []\n",
    "    page = 1\n",
    "    page_fail_retries = 0\n",
    "    consecutive_pages_failed = 0\n",
    "\n",
    "    print(\"Retrieving {}\".format(name))\n",
    "\n",
    "    while page < max_pages and consecutive_pages_failed < max_consecutive_pages_failed:\n",
    "\n",
    "        try:\n",
    "            statuses = twitter_client.user_timeline(id=name, tweet_mode=\"extended\", count=tweets_page, page=page)\n",
    "        except tweepy.TweepError:\n",
    "            statuses = None\n",
    "\n",
    "        if statuses:\n",
    "            for status in statuses:\n",
    "                tweets.append(copy_tweet(status))\n",
    "\n",
    "            page += 1\n",
    "            page_fail_retries = 0\n",
    "            consecutive_pages_failed = 0\n",
    "\n",
    "            print(\"Retrieved Tweets: {} (Pages: {}/{})\".format(len(tweets), page, max_pages), end=\"\\r\")\n",
    "            time.sleep(request_wait)\n",
    "\n",
    "        else:\n",
    "\n",
    "            if page_fail_retries < max_page_fail_retries:\n",
    "                page_fail_retries += 1\n",
    "\n",
    "                print(\"Failed to retrieve page {} attempts {}/{}, sleeping {} seconds\".format(\n",
    "                    page,\n",
    "                    page_fail_retries,\n",
    "                    max_page_fail_retries,\n",
    "                    page_fail_wait))\n",
    "                print(\"Retrieved Tweets: {} (Pages: {}/{})\".format(len(tweets), page, max_pages), end=\"\\r\")\n",
    "\n",
    "            else:\n",
    "                consecutive_pages_failed += 1\n",
    "\n",
    "                print(\"{} consecutive failed attempts on page {}, skipping page {}, (Consecutive pages failed {}/{}), sleeping {} seconds\".format(\n",
    "                    max_page_fail_retries,\n",
    "                    page, page,\n",
    "                    consecutive_pages_failed,\n",
    "                    max_consecutive_pages_failed,\n",
    "                    page_fail_wait))\n",
    "                print(\"Retrieved Tweets: {} (Pages: {}/{})\".format(len(tweets), page, max_pages), end=\"\\r\")\n",
    "\n",
    "                page += 1\n",
    "                page_fail_retries = 0\n",
    "\n",
    "            time.sleep(page_fail_wait)\n",
    "\n",
    "    print()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def get_collection(twitter_client, name):\n",
    "\n",
    "    tweets = retrieve_comments(twitter_client, name)\n",
    "\n",
    "    collection = {\n",
    "        \"collection_name\": name,\n",
    "        \"tweets\": tweets,\n",
    "        \"date\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    return collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación defino la función `get_collections` a la que se le pasa una lista de nombres de usuarios que se tienen que obtener. Primero se comprueba si la colección de tweets para ese usuario ya ha sido descargada, en tal caso la colección se lee de disco.\n",
    "\n",
    "En caso contrario se descarga la colección y se guarda en disco en formato JSON para ser leída más tarde en caso de que se vuelva a ejecutar el script.\n",
    "\n",
    "La función devuelve un diccionario con claves, una por cada usuario que se quiere obtener la colección, y valores otro diccionario que contiene una lista de tweets y algunos campos más como la fecha de descarga de los tweets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_collection_path(collection_dir, name):\n",
    "    return os.path.join(collection_dir, \"{}.json\".format(name))\n",
    "\n",
    "def get_collections(names):\n",
    "\n",
    "    twitter_client = None\n",
    "    collections = {}\n",
    "\n",
    "    os.makedirs(collections_dir, exist_ok=True)\n",
    "\n",
    "    for collection_name in names:\n",
    "\n",
    "        collection_filename = get_collection_path(collections_dir, collection_name)\n",
    "\n",
    "        if os.path.isfile(collection_filename):\n",
    "            # read collection\n",
    "\n",
    "            print(\"Using saved collection: {}\".format(collection_filename))\n",
    "            with open(collection_filename) as collection_file:\n",
    "                collection = json.load(collection_file)\n",
    "\n",
    "            print(\"  Tweets: {}\".format(len(collection[\"tweets\"])))\n",
    "\n",
    "        else:\n",
    "            # retrieve collection\n",
    "\n",
    "            if not twitter_client:\n",
    "                twitter_auth = tweepy.AppAuthHandler(twitter_api_key, twitter_api_secret)\n",
    "                twitter_client = tweepy.API(twitter_auth)\n",
    "\n",
    "            collection = get_collection(twitter_client, collection_name)\n",
    "\n",
    "            print(\"Saving collection: {}\".format(collection_filename))\n",
    "            with open(collection_filename, \"w\") as collection_file:\n",
    "                json.dump(collection, collection_file)\n",
    "\n",
    "        collections[collection_name] = collection\n",
    "\n",
    "    return collections"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se llama a la función anteriormente descrita que cargará las colecciones de tweets en la variable `twitter_collections`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twitter_collections = get_collections(similar_users + different_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función `split_training_test` a continuación se le pasa por parámetro una lista de documentos y un porcentaje de test por el que se dividirá la lista de documentos.\n",
    "La lista de documentos primero será ordenada por el campo `timestamp` para que los documentos queden ordenados por fecha.\n",
    "La función devuelve dos valores por un lado una lista que será usada como _training_ y otra lista que será usada como _test_.\n",
    "Los conjuntos devueltos han sido divididos de forma que en el conjunto de test queden los tweets más recientes.\n",
    "\n",
    "La función `extract_corpus` toma como parámetro las colecciones de tweets y de las colecciones extrae, filtra y divide los documentos en dos conjuntos, simplemente listas de texto.\n",
    "En la función se descartan los retweets porque no son contenido escrito por el usuario seleccionado sino que son textos que el usuario a decidido compartir en su _timeline_.\n",
    "Por otro lado también se descartan los tweets que después de eliminar el contenido extra que está incluido en el texto pero no es algo escrito por el usuario se quedan vacíos de texto.\n",
    "\n",
    "En el texto también se incluyen menciones a otros usuarios en caso de que el tweet sea una respuesta, o también puede incluir un link a una imagen en caso de que el tweet contenga una imagen.\n",
    "Los tweets incluyen un campo `display_text_range` en el que se indican el inicio y el final del texto escrito por el usuario.\n",
    "Es posible que se de que un tweet consista únicamente de una imagen en respuesta a otro tweet y que tras eliminar el contenido extra se quede vacío de texto, estos tweets serán descartados.\n",
    "\n",
    "El diccionario devuelto por esta función tiene la siguiente estructura:\n",
    "\n",
    "```\n",
    "corpus_sets = {}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_training_test(corpus, test_percentage):\n",
    "\n",
    "    corpus_sorted = list(map(\n",
    "        operator.itemgetter('text'),\n",
    "        sorted(corpus, key=operator.itemgetter('timestamp'))\n",
    "    ))\n",
    "\n",
    "    training_size = int(len(corpus_sorted) * (1-test_percentage))\n",
    "\n",
    "    return corpus_sorted[:training_size], corpus_sorted[training_size:]\n",
    "\n",
    "def extract_corpus(collections):\n",
    "\n",
    "    corpus_sets = {}\n",
    "\n",
    "    for collection in collections:\n",
    "\n",
    "        corpus = []\n",
    "\n",
    "        for tweet in collections[collection][\"tweets\"]:\n",
    "            start = tweet[\"display_text_range\"][0]\n",
    "            end = tweet[\"display_text_range\"][1]\n",
    "\n",
    "            if not tweet[\"retweeted_status\"] and (end-start) != 0:\n",
    "                corpus.append({'text': tweet[\"full_text\"][start:end], 'timestamp': tweet['created_at_timestamp']})\n",
    "\n",
    "        training, test = split_training_test(corpus, test_set_percentage)\n",
    "        corpus_sets[collection] = {'training': training, 'test': test}\n",
    "\n",
    "    return corpus_sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En la siguiente celda se llama a la función anteriormente descrita que extrae de las colecciones el corpus y lo almacena en la variable `collections_corpus_sets`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collections_corpus_sets = extract_corpus(twitter_collections)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vectorize_corpus(corpus, collections):\n",
    "\n",
    "    vectors = {'collections': collections,\n",
    "               'label_map': {},\n",
    "               'training': {'labels': []},\n",
    "               'test': {'labels': []}}\n",
    "\n",
    "    training_corpus = []\n",
    "    test_corpus = []\n",
    "    label = 0\n",
    "\n",
    "    for collection in collections:\n",
    "\n",
    "        training_corpus += corpus[collection]['training']\n",
    "        test_corpus += corpus[collection]['test']\n",
    "\n",
    "        vectors['training']['labels'] += [label for _ in range(0, len(corpus[collection]['training']))]\n",
    "        vectors['test']['labels'] += [label for _ in range(0, len(corpus[collection]['test']))]\n",
    "\n",
    "        vectors['label_map'][label] = collection\n",
    "        label += 1\n",
    "\n",
    "    random.Random(seed).shuffle(training_corpus)\n",
    "    random.Random(seed).shuffle(vectors['training']['labels'])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors['training']['vectors'] = vectorizer.fit_transform(training_corpus)\n",
    "    vectors['test']['vectors'] = vectorizer.transform(test_corpus)\n",
    "\n",
    "    return vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_users_vectors = vectorize_corpus(collections_corpus_sets, similar_users)\n",
    "\n",
    "different_users_vectors = vectorize_corpus(collections_corpus_sets, different_users)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(ground_truth, prediction):\n",
    "\n",
    "    confusion_matrix = [[0, 0], [0, 0]]\n",
    "\n",
    "    for gt, p in zip(ground_truth, prediction):\n",
    "        confusion_matrix[gt][p] += 1\n",
    "\n",
    "    return confusion_matrix\n",
    "\n",
    "def calculate_accuracy(ground_truth, prediction):\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for gt, p in zip(ground_truth, prediction):\n",
    "        if gt == p:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(ground_truth)\n",
    "\n",
    "def print_confusion_matrix_accuracy(confusion_matrix, accuracy, label_map):\n",
    "\n",
    "    print('{:<16}{:<16}{:<16}'.format(' ', label_map[0], label_map[1]))\n",
    "    print('{:<16}{:<16}{:<16}'.format(label_map[0], confusion_matrix[0][0], confusion_matrix[0][1]))\n",
    "    print('{:<16}{:<16}{:<16}  Accuracy: {:0.3f}'.format(label_map[1], confusion_matrix[1][0], confusion_matrix[1][1], accuracy))\n",
    "    print()\n",
    "\n",
    "def train_predict(vectors, *args, **kwargs):\n",
    "\n",
    "    print('Configuration:', args, kwargs)\n",
    "\n",
    "    classifier = svm.SVC(*args, **kwargs)\n",
    "    classifier.fit(vectors['training']['vectors'], vectors['training']['labels'])\n",
    "    prediction = classifier.predict(vectors['test']['vectors'])\n",
    "\n",
    "    confusion_matrix = calculate_confusion_matrix(vectors['test']['labels'], prediction)\n",
    "    accuracy = calculate_accuracy(vectors['test']['labels'], prediction)\n",
    "\n",
    "    print_confusion_matrix_accuracy(confusion_matrix, accuracy, vectors['label_map'])\n",
    "\n",
    "    return confusion_matrix, accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = train_predict(similar_users_vectors)\n",
    "_ = train_predict(different_users_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "collections_scores = {}\n",
    "\n",
    "for collection in collections_corpus_sets:\n",
    "\n",
    "    collections_scores[collection] = []\n",
    "    print('================== {:<16} =================='.format(collection))\n",
    "\n",
    "    for document in collections_corpus_sets[collection]['training'] + collections_corpus_sets[collection]['test']:\n",
    "\n",
    "        scores = analyzer.polarity_scores(document)\n",
    "        collections_scores[collection].append(scores)\n",
    "        print('pos: {:.2f}, neu: {:.2f}, neg: {:.2f}, compound: {: .2f}  {}'.format(scores['pos'],\n",
    "                                                                                    scores['neu'],\n",
    "                                                                                    scores['neg'],\n",
    "                                                                                    scores['compound'],\n",
    "                                                                                    document.replace('\\n', ' ')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('{:<16}{:<8}{:<8}{:<8}{:<8}'.format('Averages', 'pos', 'neu', 'neg', 'compound'))\n",
    "\n",
    "for collection in collections_scores:\n",
    "    print('{:<16}{:<8.2f}{:<8.2f}{:<8.2f}{:<8.2f}'.format(\n",
    "        collection,\n",
    "        sum(map(lambda s: s['pos'], collections_scores[collection])) / len(collections_scores[collection]),\n",
    "        sum(map(lambda s: s['neu'], collections_scores[collection])) / len(collections_scores[collection]),\n",
    "        sum(map(lambda s: s['neg'], collections_scores[collection])) / len(collections_scores[collection]),\n",
    "        sum(map(lambda s: s['compound'], collections_scores[collection])) / len(collections_scores[collection])\n",
    "    ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}